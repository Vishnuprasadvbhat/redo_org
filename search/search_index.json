{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Segmentation of the llms","text":"<p>After our last meeting, we found that we were all working hard on developing a suitable model for edge deployment using various state-of-the-art methods such as pruning, fine-tuning, quantizing, and distillation. However, the segmentation of LLMs was still an area that we had not explored.</p> <p>Furthermore, we discussed an existing method that could be incorporated into our research project for efficiently segmenting the LLM. The points I have expressed here are based on the research paper I have explored, but there may be other methods that outperform the proposed one.</p>"},{"location":"#current-implementation-of-neural-network","title":"Current Implementation of Neural Network","text":"<p>The current implementation of neural network are based on Dictionaries, List and Queue. Here for efficient training, inferencing and deployment we can make use of Computational graphs.  </p> <p></p> <p>We are going to replace the mathematical operations used in our neural network to the Node</p>"},{"location":"#comparision-of-anns-to-that-of-computational-graphs","title":"Comparision of ANNs to that of Computational Graphs","text":""},{"location":"#graph-representation-of-neural-network","title":"Graph representation of Neural Network:","text":"<p>Computational graphs are a fundamental concept in machine learning and deep learning, representing the sequence of operations needed to compute a mathematical function. They provide a structured way to visualize and manage how data flows through various operations, enabling efficient computation and optimization, particularly for automatic differentiation (used in backpropagation).</p> <p></p>"},{"location":"#why-are-computational-graphs-important","title":"Why Are Computational Graphs Important?","text":"<ol> <li>Automatic Differentiation: The graph structure allows for efficient computation of gradients during backpropagation by systematically applying the chain rule.</li> <li>Modularity: Complex functions can be broken down into simpler operations, which can be reused and recomposed.</li> <li>Parallelization: Since the graph structure outlines dependencies, parts of the computation that are independent can be parallelized, improving efficiency on GPUs or distributed systems.</li> <li>Optimization: Frameworks like TensorFlow and PyTorch build computational graphs dynamically or statically, allowing for optimizations like memory management and operation fusion.</li> </ol>"},{"location":"#types-of-computational-graphs","title":"Types of Computational Graphs","text":""},{"location":"#static-vs-dynamic-computational-graphs","title":"Static vs. Dynamic Computational Graphs","text":""},{"location":"#static-graphs-define-and-run","title":"Static Graphs (Define-and-Run):","text":"<p>The graph is defined once before execution (e.g., TensorFlow 1.x). Once the graph is constructed, it is executed as a whole.</p> <p>Pros: Allows for optimizations like memory reuse and graph-level optimizations.  Cons: Less flexible, requires redefinition of the graph for dynamic changes.</p>"},{"location":"#dynamic-graphs-define-by-run","title":"Dynamic Graphs (Define-by-Run):","text":"<p>The graph is constructed on-the-fly as operations are executed (e.g., PyTorch, TensorFlow 2.x).</p> <p>Pros: More flexible and intuitive, especially useful for tasks like recursive neural networks or variable-length sequences.  Cons: Can have slightly higher overhead during execution due to graph construction. vbnet</p>"},{"location":"#example-computational-graph-for-a-simple-function","title":"Example: Computational Graph for a Simple Function","text":"<p>**Suppose we have a simple function: **   - [ z = (x + y) \\cdot w ]</p> <p>Where: - ( x ), ( y ), and ( w ) are input variables. - ( + ) and ( \\cdot ) are operations.</p>"},{"location":"#corresponding-computational-graph","title":"Corresponding Computational Graph:","text":"<pre><code>x ----+\n      |\n      +----( + )----&gt; z\n      |            |\ny ----+            * \n                   |\nw -----------------+\n\n</code></pre>"},{"location":"#convert-llms-into-computational-graphs","title":"Convert LLMs into Computational Graphs:","text":""},{"location":"#breakdown-of-llm-components","title":"Breakdown of LLM Components","text":"<ul> <li>Overview: LLMs, such as GPT, are essentially stacks of transformer layers.</li> <li>Each transformer layer contains operations like:<ul> <li>Linear projections (matrix multiplications)</li> <li>Multi-head self-attention (dot products, softmax, and weighted sums)</li> <li>Layer normalization</li> <li>Activation functions (e.g., GeLU, ReLU)</li> <li>Feedforward neural networks</li> </ul> </li> <li>Representation: These operations can be represented as nodes in a computational graph.</li> </ul>"},{"location":"#representing-forward-pass","title":"Representing Forward Pass","text":"<ul> <li>Process: In the forward pass, the input tokens (word embeddings) are passed through a series of transformer layers.</li> <li>Node Representation:<ul> <li>Each layer's operations can be converted into a node in the graph.</li> <li>The output of one node (operation) flows into the next.</li> </ul> </li> <li>Self-Attention Mechanism:<ul> <li>The self-attention mechanism itself is a subgraph.</li> <li>Each step (e.g., attention scores calculation, softmax normalization) is broken down into individual operations.</li> </ul> </li> <li>Graph Interaction: The computational graph represents how each token interacts with others across layers.</li> </ul>"},{"location":"#backward-pass-backpropagation","title":"Backward Pass (Backpropagation)","text":"<ul> <li>Automatic Differentiation: The backward pass is handled by automatic differentiation.</li> <li>Gradient Computation:<ul> <li>Once the computational graph is constructed, frameworks like TensorFlow or PyTorch can automatically compute gradients for each parameter with respect to the loss.</li> <li>This is done by traversing the graph in reverse.</li> </ul> </li> <li>Efficiency: This allows for efficient training of the model, optimizing the parameters using gradient descent.</li> </ul>"},{"location":"#example-llm-layer-single-transformer-layer","title":"Example: LLM Layer (Single Transformer Layer)","text":"<ul> <li>Inputs: Token embeddings</li> <li>Operations:<ul> <li>Multi-head self-attention (with matrix multiplications, scaling, and softmax)</li> <li>Add &amp; Norm</li> <li>Feedforward network (with activation function)</li> <li>Add &amp; Norm</li> </ul> </li> <li>Outputs: Transformed embeddings</li> <li>Graph Representation: This can be represented as a directed acyclic graph (DAG), with each of these operations represented as nodes and the data flow between them as edges.</li> </ul>"},{"location":"#key-concepts-in-computational-graphs","title":"Key Concepts in Computational Graphs","text":""},{"location":"#nodes","title":"Nodes:","text":"<ul> <li>Each node in a computational graph represents a mathematical operation (e.g., addition, multiplication, activation functions) or a variable (e.g., input data, weights, biases).</li> <li>Input nodes hold the input data, and operation nodes perform functions on the data.</li> </ul>"},{"location":"#edges","title":"Edges:","text":"<ul> <li>The edges represent the flow of data between operations. They carry values (or tensors) that are passed from one operation to another.</li> <li>Edges define dependencies between nodes, ensuring that operations are computed in the correct order.</li> </ul>"},{"location":"#directed-acyclic-graph-dag","title":"Directed Acyclic Graph (DAG):","text":"<ul> <li>Computational graphs are typically directed acyclic graphs (DAGs), meaning that data flows in one direction, and there are no cycles or loops in the graph.</li> <li>This ensures that the computation proceeds from inputs to outputs without infinite recursion.</li> </ul>"},{"location":"#forward-pass","title":"Forward Pass:","text":"<ul> <li>During the forward pass, data flows through the graph from the input nodes, through the operations, and produces an output.</li> <li>This step calculates the predicted output based on the input data and model parameters.</li> </ul>"},{"location":"#backward-pass-backpropagation_1","title":"Backward Pass (Backpropagation):","text":"<ul> <li>During the backward pass, the graph is used to calculate gradients of the loss function with respect to model parameters.</li> <li>Automatic differentiation (reverse-mode differentiation) is applied by traversing the graph backward, allowing efficient gradient calculation for optimization algorithms like gradient descent.</li> </ul>"},{"location":"#use-cases-for-llm-segmentation","title":"Use Cases for LLM Segmentation","text":"<ul> <li>Distributed Training: Essential for efficiently training very large LLMs (like GPT-3 or similar models) across multiple GPUs or nodes.</li> <li>Edge Deployment: Allows parts of the model to be deployed on edge devices while keeping others centralized.</li> <li>Inference Pipelines: Facilitates faster, parallelized processing for real-time applications by segmenting the LLM.</li> </ul> <p>Basic Implementation in Colab - On Mathematic Expressions</p> <p>For more information: Calculus on Computational Graphs: Backpropagation</p>"},{"location":"graphmodel/","title":"Decoder as Graph Model","text":"<p>Based on Research Papers </p>"},{"location":"graphmodel/#same-architecture-different-representation","title":"Same Architecture, Different Representation","text":"<p>The primary focus is to build a single transformer or decoder-only architecture. Since our main task is predicting the next word, we can design an entire system that effectively processes and represents information in a graph format.</p>"},{"location":"graphmodel/#we-can-construct-a-gpt-architecture-using-existing-sources-and-frameworks-by-leveraging-the-same-frameworks-we-can-also-develop-a-graph-based-llm","title":"We can construct a GPT architecture using existing sources and frameworks. By leveraging the same frameworks, we can also develop a graph-based LLM.","text":"<p> source:mdpi</p>"},{"location":"graphmodel/#graph-to-graph-models","title":"Graph-to-Graph models","text":"<p>Transformers operate as graph-to-graph models, with sequences representing a specific instance of this capability. In the Graph-to-Graph Transformer architecture, attention weights are considered graph edges. By integrating graph edges into the attention weight calculations and predicting these edges using attention-like functions, we explicitly incorporate graphs into the latent representations learned by pre-trained Transformers.</p> <p>Furthermore, this approach introduces iterative graph refinement, creating a unified embedding of input, output, and latent graphs. This enables non-autoregressive graph prediction, optimizing the entire graph without requiring a specialized pipeline or decoding strategy. Empirical results show that this architecture achieves state-of-the-art accuracy in modeling various linguistic structures while effectively integrating with the latent linguistic representations acquired through pretraining.</p>"},{"location":"graphmodel/#building-a-graph-based-llm","title":"Building a Graph-based LLM","text":"<p> source:Graph Language Model</p> <p>While Language Models (LMs) are essential for Natural Language Processing (NLP), their interaction with structured knowledge graphs (KGs) remains an area of active research. Current methods either linearize KGs for embedding with LMs, which neglects structural information, or utilize Graph Neural Networks (GNNs), which fail to capture text features as effectively as pretrained LMs.</p> <p>This work introduces a new model called the Graph Language Model (GLM), which combines the advantages of both approaches while addressing their limitations. The GLM is initialized from a pretrained LM to improve comprehension of individual graph concepts and relationships. Its architecture incorporates graph biases to facilitate effective knowledge distribution.</p> <p>As a result, GLMs can process graphs, texts, and combined inputs. Empirical evaluations in relation classification tasks indicate that GLM embeddings outperform both LM- and GNN-based baselines in supervised and zero-shot settings, demonstrating their versatility.</p>"},{"location":"graphmodel/#supporting-papers","title":"Supporting Papers:","text":"<ul> <li> <p>Graph Language Model</p> </li> <li> <p>Transformers as Graph Model</p> </li> <li> <p>Optimizing Graph using Swarm</p> </li> </ul>"},{"location":"graphmodel/#reference","title":"Reference:","text":"<ul> <li>Computational Graphs for Neural Networks</li> <li>Minimal Code</li> <li>Example code</li> </ul>"},{"location":"implement/","title":"Implementation of the GNN for efficient Segmentation","text":"<p>c</p>"},{"location":"paper/","title":"Segment the LLM using Subgraphs","text":"<p>Segmenting an LLM involves dividing its computational graph into smaller, independent parts, typically to enable parallel execution, distributed inference, or efficient model deployment across different hardware resources. This is useful, especially when working with extremely large models that are too resource-intensive to run on a single device or need optimization for specific deployment scenarios (e.g., edge devices or multi-GPU setups).</p>"},{"location":"paper/#based-on-the-paper-linkedlingual","title":"Based on the paper LinkedLingual:","text":"<p>  source:LinkedLingual</p>"},{"location":"paper/#adoptable-section","title":"Adoptable Section","text":""},{"location":"paper/#subgraph-extraction-from-llms","title":"Subgraph Extraction from LLMs","text":"<p> source:LinkedLingual</p>"},{"location":"paper/#methods-to-segment-llms-using-computational-graphs","title":"Methods to Segment LLMs Using Computational Graphs","text":"<p> source:Parellelism</p>"},{"location":"paper/#1-pipeline-parallelism-layer-wise-segmentation","title":"1. Pipeline Parallelism (Layer-Wise Segmentation)","text":"<ul> <li>Description: The LLM is segmented by dividing its layers across different devices or nodes.</li> <li>Process:</li> <li>The computational graph is split at the boundaries between layers.</li> <li>Each segment is processed sequentially by different hardware components (e.g., GPUs or TPUs).</li> <li>Example: In a transformer model, assign layers 1-6 to GPU 1 and layers 7-12 to GPU 2.</li> </ul>"},{"location":"paper/#how-it-works","title":"How it Works:","text":"<ul> <li>In the forward pass, input data is processed layer-by-layer, with each segment handled by a different device.</li> <li>Once one segment finishes, the next device picks up the output from the previous one, like an assembly line.</li> <li>In the backward pass, gradients are propagated similarly.</li> </ul>"},{"location":"paper/#benefits","title":"Benefits:","text":"<ul> <li>Reduces memory footprint per device.</li> <li>Allows the use of multiple devices in parallel, improving scalability.</li> </ul>"},{"location":"paper/#challenges","title":"Challenges:","text":"<ul> <li>Communication overhead between devices can slow down training or inference.</li> <li>Latency due to synchronization between segments.</li> </ul>"},{"location":"paper/#2-tensor-parallelism-within-layer-segmentation","title":"2. Tensor Parallelism (Within-Layer Segmentation)","text":"<ul> <li>Description: Tensor parallelism divides the operations within a layer instead of segmenting by layers.</li> <li>Process:</li> <li>Large tensors (e.g., weight matrices in self-attention or feedforward layers) are split across multiple devices.</li> <li>Example: For a large matrix multiplication, split the weight matrix into smaller blocks and distribute them across GPUs.</li> </ul>"},{"location":"paper/#how-it-works_1","title":"How it Works:","text":"<ul> <li>Each device performs its portion of the tensor operation simultaneously.</li> <li>The partial results from each device are combined at the end of the operation (e.g., using all-reduce operations).</li> </ul>"},{"location":"paper/#benefits_1","title":"Benefits:","text":"<ul> <li>Increases parallelism within each layer, speeding up computation.</li> <li>Effective for very large models where a single layer is too large to fit into the memory of a single device.</li> </ul>"},{"location":"paper/#challenges_1","title":"Challenges:","text":"<ul> <li>Synchronization and communication between devices can introduce overhead.</li> <li>May require sophisticated partitioning strategies to ensure efficient memory usage and load balancing.</li> </ul>"},{"location":"paper/#3-model-sharding-distributed-across-different-devices","title":"3. Model Sharding (Distributed Across Different Devices)","text":"<ul> <li>Description: Different parts of the computational graph are distributed across heterogeneous devices (e.g., CPU, GPU, edge devices).</li> <li>Process:</li> <li>Specific segments of the model run on devices optimized for their computation.</li> <li>Example: Run the early layers of a transformer model on a powerful cloud GPU and the later layers on edge devices for faster localized inference.</li> </ul>"},{"location":"paper/#how-it-works_2","title":"How it Works:","text":"<ul> <li>The computational graph is segmented based on device capability, memory, and power requirements.</li> <li>Certain segments are offloaded to appropriate hardware (e.g., CPU for lightweight computation, GPU for heavy tensor operations).</li> </ul>"},{"location":"paper/#benefits_2","title":"Benefits:","text":"<ul> <li>Enables the use of edge devices or other constrained hardware in distributed environments.</li> <li>Can reduce communication latency if segments are placed close to where data is generated or consumed.</li> </ul>"},{"location":"paper/#challenges_2","title":"Challenges:","text":"<ul> <li>Requires efficient management of communication between heterogeneous devices.</li> <li>Segmenting must account for device-specific performance characteristics, such as memory capacity and computation speed.</li> </ul>"},{"location":"paper/#4-task-specific-segmentation","title":"4. Task-Specific Segmentation","text":"<ul> <li>Description: LLMs can be segmented based on specific tasks.</li> <li>Process:</li> <li>Task-specific layers can be offloaded to different hardware resources while shared layers remain on a central server.</li> <li>Use Case: Particularly useful for multi-task learning or when deploying a model that handles various related tasks.</li> </ul>"},{"location":"paper/#5-graph-partitioning","title":"5. Graph Partitioning","text":"<ul> <li>Description: Some deep learning frameworks allow automatic partitioning of computational graphs based on hardware constraints.</li> <li>Process:</li> <li>Analyze the computational graph and strategically split it into segments that can be executed independently or in parallel.</li> </ul>"},{"location":"paper/#frameworks-and-tools-for-segmenting-llms","title":"Frameworks and Tools for Segmenting LLMs","text":"<ul> <li>DeepSpeed (Microsoft): Provides tools for pipeline parallelism and tensor parallelism, enabling segmentation of large models across multiple GPUs.</li> <li>TensorFlow: Offers APIs for distributing computation across devices, facilitating segmentation using strategies like model parallelism and data parallelism.</li> <li>PyTorch: The <code>torch.distributed</code> library allows flexible partitioning of the computational graph and distribution across multiple devices.</li> <li>Hugging Face Transformers: The <code>Accelerate</code> library provides tools to split large models across multiple GPUs or TPUs for efficient training and inference.</li> <li>ONNX Runtime: Allows ONNX models to be split into segments and optimized for different hardware, suitable for distributed deployment or model partitioning.</li> </ul>"},{"location":"paper/#preparation-for-mobile-deployment","title":"Preparation for Mobile Deployment:","text":"<ul> <li>Segment into Subgraphs: The graphs are divided into smaller, independent subgraphs that can work separately on different devices.</li> <li>Key Node Identification: </li> <li>Nodes that take inputs from a single source and provide outputs to multiple nodes are identified as key points for creating subgraphs.</li> <li>These nodes usually represent distinct layers or operations in the model, making them suitable for independent execution.</li> </ul>"},{"location":"paper/#subgraph-dependency-search","title":"Subgraph Dependency Search:","text":"<ul> <li>Dependency Management: To manage connections between nodes in different subgraphs, a dependency search algorithm is employed.</li> <li>Two Key Maps:</li> <li>Residual Dependency Map (RDM): <ul> <li>Tracks dependencies between non-adjacent subgraphs.</li> <li>Identifies when a subgraph relies on nodes from an earlier, but not directly preceding, subgraph.</li> </ul> </li> <li>Sequential Dependency Map (SDM):<ul> <li>Monitors direct dependencies between adjacent subgraphs.</li> <li>Ensures outputs from one subgraph are used as inputs for the next subgraph.</li> </ul> </li> </ul>"},{"location":"paper/#model-assignment-optimization","title":"Model Assignment Optimization:","text":"<ul> <li>Assign Subgraphs to Mobile Devices: After segmenting LLMs into subgraphs, the next step is to allocate these subgraphs as executable modules on mobile devices.</li> <li>Consider Device Constraints: The assignment process takes into account device limitations to minimize computation and data transmission times.</li> <li>Profiling and Optimization:</li> <li>Subgraphs are compiled into sub-modules and profiled for:<ul> <li>FLOP (Floating Point Operations) count.</li> <li>Memory requirements.</li> <li>Data output size.</li> </ul> </li> <li>A primary optimizer formulates a linear optimization problem to balance local computation and data transmission.</li> <li>Constraints are set to ensure memory usage on each device does not exceed a predetermined limit of the device's available memory.</li> </ul>"},{"location":"paper/#key-points-from-the-linkedlingual","title":"Key points from the LinkedLingual:","text":"<ul> <li> <p>Challenge: Deploying Large Language Models (LLMs) locally on mobile devices is difficult due to high memory requirements.</p> </li> <li> <p>Solution: LinguaLinked is introduced as a system for decentralized, distributed LLM inference on mobile devices.</p> </li> <li> <p>Data Privacy: The system processes information locally, ensuring data privacy by preventing data from leaving the device.</p> </li> <li> <p>Key Strategies:</p> </li> <li>Optimized Model Assignment:<ul> <li>Segments LLMs and employs linear optimization to align segments with the capabilities of each device.</li> </ul> </li> <li>Optimized Data Transmission:<ul> <li>Ensures efficient and structured data flow between model segments while preserving the integrity of the original model structure.</li> </ul> </li> <li> <p>Runtime Load Balancer:</p> <ul> <li>Actively monitors and redistributes tasks among mobile devices to prevent bottlenecks, enhancing overall efficiency and responsiveness.</li> </ul> </li> <li> <p>Testing Results:</p> <ul> <li>Extensive testing demonstrates that LinguaLinked supports efficient LLM inference with consistent throughput and minimal latency across various mobile devices, including both high-end and low-end Android devices.</li> </ul> </li> </ul>"},{"location":"paper/#references","title":"References","text":"<ul> <li>LinkedLingual </li> <li>Parellelism</li> </ul>"}]}